{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCqNQf3DsoRv3h5s06Xjuf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SACHINSIROHI47/DataScience_with_Python_Programming_Project/blob/main/Backpropagation_in_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Backpropagation in Neural Network**\n",
        "\n",
        "Backpropagation (short for \"Backward Propagation of Errors\") is a method used to train artificial neural networks. Its goal is to reduce the difference between the modelâ€™s predicted output and the actual output by adjusting the weights and biases in the network.\n",
        "\n",
        "\n",
        "Backpropagation is a machine learning and artificial intelligence algorithm that trains neural networks to improve their predictions by adjusting their weights and biases. It works by analyzing the error rate from the previous iteration and adjusting the weights to reduce the error.\n",
        "\n",
        "Here's how backpropagation works:\n",
        "\n",
        "Calculate the loss function: The computer calculates the difference between the input and the output, which is known as the loss function.\n",
        "\n",
        "**Work backward**: Backpropagation works backward from the output nodes to the input nodes.\n",
        "\n",
        "**Adjust the weights**: Backpropagation uses the chain rule to calculate how changes to each parameter affect the overall error. It then adjusts the weights and biases to minimize the error.\n",
        "\n",
        "Backpropagation is a standard method for training artificial neural networks. It's used in many neural networks, including Convolutional Neural Networks (CNNs)."
      ],
      "metadata": {
        "id": "8zSCLeS59FtF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efu1eZtu8o1T",
        "outputId": "5796e345-0281-4ead-dcd9-698e47c3a187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error:0.4964100319027255\n",
            "Error:0.008584525653247157\n",
            "Error:0.0057894598625078085\n",
            "Error:0.004629176776769985\n",
            "Error:0.0039587652802736475\n",
            "Error:0.003510122567861678\n",
            "Output After Training:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00260572],\n",
              "       [0.99672209],\n",
              "       [0.99701711],\n",
              "       [0.00386759]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "# Input dataset\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])\n",
        "\n",
        "# Output dataset\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Initialize weights randomly with mean 0\n",
        "np.random.seed(1)\n",
        "weights0 = 2 * np.random.random((3,4)) - 1\n",
        "weights1 = 2 * np.random.random((4,1)) - 1\n",
        "\n",
        "# Training loop\n",
        "for i in range(60000):\n",
        "\n",
        "  # Forward propagation\n",
        "  layer0 = X\n",
        "  layer1 = sigmoid(np.dot(layer0, weights0))\n",
        "  layer2 = sigmoid(np.dot(layer1, weights1))\n",
        "\n",
        "  # Calculate error\n",
        "  layer2_error = y - layer2\n",
        "\n",
        "  if (i% 10000) == 0:\n",
        "    print (\"Error:\" + str(np.mean(np.abs(layer2_error))))\n",
        "\n",
        "  # Backpropagation\n",
        "  layer2_delta = layer2_error * sigmoid_derivative(layer2)\n",
        "  layer1_error = layer2_delta.dot(weights1.T)\n",
        "  layer1_delta = layer1_error * sigmoid_derivative(layer1)\n",
        "\n",
        "  # Update weights\n",
        "  weights1 += layer1.T.dot(layer2_delta)\n",
        "  weights0 += layer0.T.dot(layer1_delta)\n",
        "\n",
        "\n",
        "print(\"Output After Training:\")\n",
        "layer2\n"
      ]
    }
  ]
}