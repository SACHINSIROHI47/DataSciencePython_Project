# -*- coding: utf-8 -*-
"""NLP-SachinSirohi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TO99UzEX2Wgfo3FJF4QVusYXjG18j1UO
"""

!ls

import os

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets list

import os

os.environ ['KAGGLE_CONFIG_DIR'] = '.'

!kaggle competitions download -c quora-insincere-questions-classification -f train.csv -p data

IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ



if IS_KAGGLE:
    data_dir = '../input/quora-insincere-questions-classification'
    train_fname = data_dir + '/train.csv'
    test_fname = data_dir + '/test.csv'
    sample_fname = data_dir + '/sample_submission.csv'
else:
    os.environ['KAGGLE_CONFIG_DIR'] = '.'
    !kaggle competitions download -c quora-insincere-questions-classification -f train.csv -p data
    !kaggle competitions download -c quora-insincere-questions-classification -f test.csv -p data
    !kaggle competitions download -c quora-insincere-questions-classification -f sample_submission.csv -p data
    train_fname = 'data/train.csv.zip'
    test_fname = 'data/test.csv.zip'
    sample_fname = 'data/sample_submission.csv.zip'

"""#**Data Loading of Three DataSet**"""

import pandas as pd
raw_df = pd.read_csv(train_fname)
raw_df

"""#Read Train Dataset"""

sincere_df = raw_df[raw_df.target == 0]
sincere_df.question_text.values[:10]
insincere_df = raw_df[raw_df.target == 1]
insincere_df.question_text.values[:10]
raw_df.target.value_counts(normalize=True)

raw_df.target.value_counts(normalize=True).plot(kind='bar')

"""# Load Test Dataset"""

test_df = pd.read_csv(test_fname)
test_df

"""# Load Test Sample"""

test_df = pd.read_csv(test_fname)
test_df

sub_df = pd.read_csv(sample_fname)
sub_df

sub_df.prediction.value_counts()

"""Create a Working Sample

# Create a Working Sample
"""

if IS_KAGGLE:
    SAMPLE_SIZE = len(raw_df)
else:
    SAMPLE_SIZE = 100_000

sample_df = raw_df.sample(SAMPLE_SIZE, random_state=42)
sample_df

"""#Text Preprocessing Techniques
Outline:

Understand the bag of words model
Tokenization
Stop word removal
Stemming
Bag of Words Intuition
Create a list of all the words across all the text documents
You convert each document into vector counts of each word
Limitations:

#There may be too many words in the dataset
Some words may occur too frequently
Some words may occur very rarely or only once
A single word may have many forms (go, gone, going or bird vs. birds)
"""

q0 = sincere_df.question_text.values[1]
q0

q1 = raw_df[raw_df.target == 1].question_text.values[0]
q1

"""# Tokenization
splitting a document into words and separators
"""

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

q0

word_tokenize(q0)

word_tokenize(' this is (something) with, a lot of, punctuation;')

q1

word_tokenize(q1)

q0_tok = word_tokenize(q0)
q1_tok = word_tokenize(q1)

"""# Stop Word RemovalÂ¶
Removing commonly occuring words
"""

q1_tok

from nltk.corpus import stopwords
nltk.download('stopwords')
english_stopwords = stopwords.words('english')

", ".join(english_stopwords)

def remove_stopwords(tokens):
    return [word for word in tokens if word.lower() not in english_stopwords]

q0_tok

q0_stp = remove_stopwords(q0_tok)

q0_stp

q1_stp = remove_stopwords(q1_tok)

q1_tok

q1_stp

"""#Stemming
"go", "gone", "going" -> "go" "birds", "bird" -> "bird"
"""

from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer(language='english')

stemmer.stem('going')

stemmer.stem('supposedly')

q0_stm = [stemmer.stem(word) for word in q0_stp]

q0_stp

q0_stm

q1_stm = [stemmer.stem(word) for word in q1_stp]

q1_stp

q1_stm

"""#Lemmatization
"love" -> "love" "loving" -> "love" "lovable" -> "love"

#Implement Bag of Words
Outline:



1.   Create a vocabulary using Count Vectorizer
2.   Create a vocabulary using Count Vectorizer
3.   Configure text preprocessing in Count Vectorizer
"""

sample_df

small_df = sample_df[:5]
small_df



small_df.question_text.values

from sklearn.feature_extraction.text import CountVectorizer
small_vect = CountVectorizer()
small_vect.fit(small_df.question_text)



"""CountVectorizer()
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
"""

small_vect.vocabulary_

small_vect.get_feature_names_out()



"""Transform documents into Vectors"""

vectors = small_vect.transform(small_df.question_text)
vectors.shape

small_df.question_text.values[0]

vectors[0].toarray()

vectors.toarray()

"""Learn More in this link

https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
"""



"""ML Models for Text Classification

Outline:


*   Create a training & validation set
*   Train a logistic regression model
*   Make predictions on training, validation & test data

Split into Training and Validation Set
"""



sample_df



"""Second Example"""

from sklearn.feature_extraction.text import CountVectorizer
corpus = [
'This is the first document.',
'This document is the second document.',
'And this is the third one.',
'Is this the first document?']